{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcC8VtIk32MM"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "import your life\n",
        "'''\n",
        "import numpy as np\n",
        "import io\n",
        "import datetime as date\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_outliers_with_nan(df, column,no0=True,bounds=True,lowerbound=''):\n",
        "  \"\"\"\n",
        "  Edits a dataframe terns outliers (useing the inerqotil rang) and zeros in to NAs. If it is unwanted to thern zeros into na no0=False.\n",
        "\n",
        "  Returns:\n",
        "  - df edited.\n",
        "  \"\"\"\n",
        "  df[column] = pd.to_numeric(df[column], errors='coerce') #makeing everthing pd readable\n",
        "  if no0: #If we are wating to drop the 0'z\n",
        "      df.loc[df[column] == 0, column] = np.nan\n",
        "    # Calculate quantiles and IQR\n",
        "  df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "  q1 = np.nanquantile(df[column], 0.25)\n",
        "  q3 = np.nanquantile(df[column], 0.75)\n",
        "  iqr = q3 - q1\n",
        "    # Calculate upper and lower bounds\n",
        "  upper_bound = q3 + (1.5 * iqr)\n",
        "  lower_bound = q1 - (1.5 * iqr)\n",
        "  if bounds: #If we are wanting to control wher the lower bound is (needs to be retooled for upper_bound)\n",
        "      # Replace outliers with NaN\n",
        "    df.loc[df[column] < lower_bound, column] = np.nan\n",
        "    df.loc[df[column] > upper_bound, column] = np.nan\n",
        "  else:\n",
        "    df.loc[df[column] < lowerbound, column] = np.nan\n",
        "    df.loc[df[column] > upper_bound, column] = np.nan\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "hrkumCspFFDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Main = pd.read_excel('data-SixForksOnrampMainlineVol-Apr25-May29.xlsx') #reads in main line data\n",
        "print(Main)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QK2ZY3H1vhM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "On14=pd.read_csv('data-SixForksOnrampPassageVolume-May14-29.csv') #reads in Onramp data"
      ],
      "metadata": {
        "id": "d5rDNzNJwVex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "On14=On14.drop('speed',axis='columns')\n",
        "On14\n"
      ],
      "metadata": {
        "id": "NRy6lSrkypyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "On14['volume']=On14['volume']*12 #this is to akont for the when the data is being aerated and the time period looks to be 15 minutes"
      ],
      "metadata": {
        "id": "cR75Q3p7dhtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "On25=pd.read_csv('data-Six Forks Onramp Passage Volume-Apr25toMay14.csv')\n",
        "On25.head(3726)\n",
        "On25_1 = On25.iloc[:3724]  #seprats at the different time agerashons\n",
        "On25_2 = On25.iloc[3724:]\n",
        "#3724\n",
        "On25_1['volume'] =On25_1['volume'] *12 #time period looks to be 5 minutes\n",
        "On25_2[\"volume\"] = On25_2[\"volume\"] * 2 #time period looks to be 30 minutes"
      ],
      "metadata": {
        "id": "RZiA5rt5w9dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "On22=pd.read_csv('Six Forks Onramp Passage Volume 5-min-5-29to7-15.csv')\n",
        "On22['volume'] = On22['volume']*12\n",
        "On22=On22.drop('speed',axis='columns')"
      ],
      "metadata": {
        "id": "yM1mYsak_QSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "On=pd.concat([On14, On25_1,On25_2,On22], ignore_index=True, axis=0,join=\"inner\") #conbining the 4 time periods\n",
        "On"
      ],
      "metadata": {
        "id": "kxo0mAtbzSt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Occ25=pd.read_csv('data-Six Forks Onramp Queue Occ-Apr25toMay14.csv') #reads in the Queue occupancy\n",
        "Occ14=pd.read_csv('data-SixForksOnrampQueueOcc-May14-29.csv')\n",
        "Occ22=pd.read_csv('Six Forks Onramp Queue Occ 1-min-5-29to7-16.csv')"
      ],
      "metadata": {
        "id": "7gRiwVM_xLzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Occ=pd.concat([Occ14,Occ25,Occ22],ignore_index=True,axis=0,join=\"inner\")\n",
        "Occ=Occ.drop(['volume','speed'],axis=1)"
      ],
      "metadata": {
        "id": "RVdtMGWyzwLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Occ['timestamp'] = pd.to_datetime(Occ['timestamp']) #converting to dt\n",
        "Occ['timestamp_local'] = Occ['timestamp'].dt.tz_convert('US/Eastern').dt.tz_localize(None) #locilsing\n",
        "On['timestamp'] = pd.to_datetime(On['timestamp'])\n",
        "On['timestamp_local'] = On['timestamp'].dt.tz_convert('US/Eastern').dt.tz_localize(None)\n",
        "Occ = Occ.sort_values(by='timestamp_local')\n",
        "Occ"
      ],
      "metadata": {
        "id": "einbqYA2jfFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Occ['detector_group_id']=\"Onramp Queue\"\n",
        "On['detector_group_id']=\"Onramp Passage\"\n",
        "Occ.rename(columns = {'occupancy':'occupancy Occ','detector_group_id':'detector_group'}, inplace = True) #renaming everthing for the joins\n",
        "On.rename(columns={'occupancy':'occupancy On','volume':'volume On','detector_group_id':'detector_group'}, inplace = True) #renaming everthing for the joins\n",
        "Occ=Occ.drop(['timestamp'],axis=1)\n",
        "On=On.drop(['timestamp'],axis=1)\n",
        "#Occ 49056\n",
        "#On #8318\n",
        "#df 3287"
      ],
      "metadata": {
        "id": "V1BLVviH1aj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Main"
      ],
      "metadata": {
        "id": "o64FPT5U1Bu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding the new days\n",
        "Main22=pd.read_csv('AGG-Six Forks Mainline VOS 15-min-5-29to7-24.csv')\n",
        "Main22"
      ],
      "metadata": {
        "id": "i6jBoN8I0VRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Main22['timestamp'] = pd.to_datetime(Main22['timestamp'])\n",
        "Main22['timestamp'] = Main22['timestamp'].dt.tz_convert('US/Eastern').dt.tz_localize(None)\n",
        "Main22.rename(columns = {'occupancy':'occupancy (pct)','volume':'volume (vph)','speed':'speed (mph)','timestamp':'timestamp_local'}, inplace = True)\n",
        "Main22=Main22.drop(['detector_group_id'],axis=1) #wich decter am I filtering for or should use all of them\n",
        "Main22['volume (vph)'] = Main22['volume (vph)']*4"
      ],
      "metadata": {
        "id": "_-8GHSeK3W7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Main22"
      ],
      "metadata": {
        "id": "bYWb6U08IhvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Main=pd.concat([Main,Main22],ignore_index=True,axis=0,join=\"inner\")\n",
        "Main= Main.sort_values(by='timestamp_local')\n",
        "Main"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MUrA41044jvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "fig = px.line(Main, x='timestamp_local', y='volume (vph)')\n",
        "fig.show()\n",
        "#volume (vph){5/29 1:45-6/6 21:30}\n",
        "#occupancy (pct){5/29 2:45-6/6 21:30}\n",
        "#speed (mph){,5/29 1:45-6/6 21:30}"
      ],
      "metadata": {
        "id": "QexRbe90KG9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = pd.merge(left=On,right=Occ,how=\"left\",on='timestamp_local').fillna(\"\") #mering 1\n",
        "df = pd.merge(left=temp,right=Main,how=\"left\",on='timestamp_local').fillna(\"\") #merging 2\n",
        "df"
      ],
      "metadata": {
        "id": "DLWpWtC_gGwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go #cheking for down times\n",
        "fig = go.Figure()\n",
        "#What is with the weredd gap in main?\n",
        "fig.add_trace(go.Scatter(x=df['timestamp_local'], y=df['volume On'], mode='lines', name='volume On'))\n",
        "fig.add_trace(go.Scatter(x=df['timestamp_local'], y=df['occupancy On'], mode='lines', name='occupancy On'))\n",
        "fig.add_trace(go.Scatter(x=df['timestamp_local'], y=df['occupancy Occ'], mode='lines', name='occupancy Occ'))\n",
        "#'volume On'{4/25 23:00-4/25 10:30,5/29 1:30-6/6 21:35}\n",
        "#occupancy On'{4/25 23:00-4/25 10:30, 5/29 1:30-6/6 21:35}#with some data errors of greater then 100\n",
        "#occupancy Occ{5/29 1:30-6/6 21:35}#with some data errors of greater then 100\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "PahpijOmRW2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['weekday'] = df['timestamp_local'].dt.weekday #adds a weekdate column where 0 Monday and Sunday is 6"
      ],
      "metadata": {
        "id": "j8IQqB8iriv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AMPeak={}\n",
        "for index, row in df.iterrows():\n",
        "  if df['weekday'].iloc[index] < 5:\n",
        "    if df['timestamp_local'].iloc[index].hour >= 7 and df['timestamp_local'].iloc[index].hour < 9:\n",
        "      AMPeak[index] = True\n",
        "    else:\n",
        "        AMPeak[index] = np.nan\n",
        "  else:\n",
        "        AMPeak[index] = np.nan\n",
        "df['AMPeak']= pd.Series(AMPeak) #adds col based if it is a weekday and if it is between the hors of 7AM to 9AM"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KW6MYhE1us8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['time'] = df['timestamp_local'].dt.strftime('%H:%M') #Adding a new column Time"
      ],
      "metadata": {
        "id": "ZLEiTvQi-jDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = {}\n",
        "for time, group in df.groupby('time'): #groups by time and then makes separate dataframe in a dictionary for each time\n",
        "    group = group.drop(columns=['time'])\n",
        "    dfs[time] = group"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ANDNPYfp98oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for time in dfs:\n",
        "  dfs[time] = replace_outliers_with_nan(dfs[time], 'volume On') #replaceing all outliers with na\n",
        "  dfs[time] = replace_outliers_with_nan(dfs[time], 'occupancy On',no0=False,bounds=False,lowerbound=0)\n",
        "  dfs[time] = replace_outliers_with_nan(dfs[time], 'occupancy Occ',no0=False,bounds=False,lowerbound=0)\n",
        "  dfs[time] = replace_outliers_with_nan(dfs[time], 'volume (vph)')\n",
        "  dfs[time] = replace_outliers_with_nan(dfs[time], 'occupancy (pct)', no0=False,bounds=False,lowerbound=0)\n",
        "  dfs[time] = replace_outliers_with_nan(dfs[time], 'speed (mph)')\n"
      ],
      "metadata": {
        "id": "f6luk35ESPV_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.concat(dfs.values())"
      ],
      "metadata": {
        "id": "wKBs7mToLlZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOS = {}\n",
        "for c, row in df2.iterrows(): #crating bins for speed and other basted on predfid values\n",
        "    this = df2['speed (mph)'].loc[c]\n",
        "\n",
        "    if not pd.isnull(this):  # Check for NaN values\n",
        "        try:\n",
        "            this = float(this)\n",
        "            if this == 0:\n",
        "                LOS[c] = 0\n",
        "            elif this <= 20:\n",
        "                LOS[c] = \"20 or less\"\n",
        "            elif this <= 30:\n",
        "                LOS[c] = \"20's\"\n",
        "            elif this <= 40:\n",
        "                LOS[c] = \"30's\"\n",
        "            elif this <= 50:\n",
        "                LOS[c] = \"40's\"\n",
        "            elif this <= 60:\n",
        "                LOS[c] = \"50's\"\n",
        "            elif this <= 70:\n",
        "                LOS[c] = \"60's\"\n",
        "            elif this <= 80:\n",
        "                LOS[c] = \"70's\"\n",
        "            elif this <= 90:\n",
        "                LOS[c] = \"80's\"\n",
        "            else:\n",
        "                LOS[c] = \"Greater than 90\"\n",
        "        except ValueError:\n",
        "            LOS[c] = '' # Handle cases where conversion to float fails\n",
        "    else:\n",
        "        LOS[c] = ''  # Handle NaN values\n",
        "\n",
        "df2['Speedgroup'] = pd.Series(LOS)"
      ],
      "metadata": {
        "id": "JBkQGbzK1jP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOS = {}\n",
        "for c, row in df2.iterrows():\n",
        "    this = df2['volume (vph)'].loc[c]\n",
        "\n",
        "    if not pd.isnull(this):  # Check for NaN values\n",
        "        try:\n",
        "            this = float(this)\n",
        "            if this == 0:\n",
        "                LOS[c] = 0\n",
        "            elif this <= 1000:\n",
        "                LOS[c] = \"1000\"\n",
        "            elif this <= 1500:\n",
        "                LOS[c]= \"1500\"\n",
        "            elif this <= 2000:\n",
        "                LOS[c] = \"2000\"\n",
        "            elif this <= 2500:\n",
        "                LOS[c] = \"2500\"\n",
        "            elif this <= 3000:\n",
        "                LOS[c] = \"3000\"\n",
        "            elif this <= 3500:\n",
        "                LOS[c] = \"3500\"\n",
        "            elif this <= 4000:\n",
        "                LOS[c] = \"4000\"\n",
        "            else:\n",
        "                LOS[c] = \"Greater than 4000\"\n",
        "        except ValueError:\n",
        "            LOS[c] = '' # Handle cases where conversion to float fails\n",
        "    else:\n",
        "        LOS[c] = ''  # Handle NaN values\n",
        "\n",
        "df2['Volume bins'] = pd.Series(LOS)"
      ],
      "metadata": {
        "id": "Twd3z7Jg8Gnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOS = {}\n",
        "for c, row in df2.iterrows():\n",
        "    this = df2['volume On'].loc[c]\n",
        "\n",
        "    if not pd.isnull(this):  # Check for NaN values\n",
        "        try:\n",
        "            this = float(this)\n",
        "            if this == 0:\n",
        "                LOS[c] = 0\n",
        "            elif this <= 500:\n",
        "                LOS[c] = \"500\"\n",
        "            elif this <= 750:\n",
        "                LOS[c] = \"750\"\n",
        "            elif this <= 1000:\n",
        "                LOS[c] = \"1000\"\n",
        "            elif this <= 1250:\n",
        "                LOS[c]= \"1250\"\n",
        "            elif this <= 1500:\n",
        "                LOS[c] = \"1500\"\n",
        "            elif this <= 1750:\n",
        "                LOS[c] = \"1750\"\n",
        "            elif this <= 2000:\n",
        "                LOS[c] = \"2000\"\n",
        "            elif this <= 2500:\n",
        "                LOS[c] = \"2500\"\n",
        "            elif this <= 3000:\n",
        "                LOS[c] = \"3000\"\n",
        "            elif this <= 3500:\n",
        "                LOS[c] = \"3500\"\n",
        "            elif this <= 4000:\n",
        "                LOS[c] = \"4000\"\n",
        "            else:\n",
        "                LOS[c] = \"Greater than 4000\"\n",
        "        except ValueError:\n",
        "            LOS[c] = '' # Handle cases where conversion to float fails\n",
        "    else:\n",
        "        LOS[c] = ''  # Handle NaN values\n",
        "\n",
        "df2['Volume Bins On'] = pd.Series(LOS)"
      ],
      "metadata": {
        "id": "CNdXziDytZqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOS = {}\n",
        "for c, row in df2.iterrows():\n",
        "    this = df2['occupancy On'].loc[c]\n",
        "\n",
        "    if not pd.isnull(this):  # Check for NaN values\n",
        "        try:\n",
        "            this = float(this)\n",
        "            if this == 0:\n",
        "                LOS[c] = 0\n",
        "            elif this <= 10:\n",
        "                LOS[c] = \"10%\"\n",
        "            elif this <= 20:\n",
        "                LOS[c] = \"20%\"\n",
        "            elif this <= 30:\n",
        "                LOS[c] = \"30%\"\n",
        "            elif this <= 40:\n",
        "                LOS[c] = \"40%\"\n",
        "            elif this <= 50:\n",
        "                LOS[c] = \"50%\"\n",
        "            elif this <= 60:\n",
        "                LOS[c] = \"60%\"\n",
        "            elif this <= 70:\n",
        "                LOS[c] = \"70%\"\n",
        "            elif this <= 80:\n",
        "                LOS[c] = \"80%\"\n",
        "            elif this <= 90:\n",
        "                LOS[c] = \"90%\"\n",
        "            else:\n",
        "                LOS[c] = \"100%\"\n",
        "        except ValueError:\n",
        "            LOS[c] = '' # Handle cases where conversion to float fails\n",
        "    else:\n",
        "        LOS[c] = ''  # Handle NaN values\n",
        "\n",
        "df2['occupancy On Bins'] = pd.Series(LOS)"
      ],
      "metadata": {
        "id": "O2upNZZ4tnfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOS = {}\n",
        "for c, row in df2.iterrows():\n",
        "    this = df2['occupancy (pct)'].loc[c]\n",
        "\n",
        "    if not pd.isnull(this):  # Check for NaN values\n",
        "        try:\n",
        "            this = float(this)\n",
        "            if this == 0:\n",
        "                LOS[c] = 0\n",
        "            elif this <= 10:\n",
        "                LOS[c] = \"10%\"\n",
        "            elif this <= 20:\n",
        "                LOS[c] = \"20%\"\n",
        "            elif this <= 30:\n",
        "                LOS[c] = \"30%\"\n",
        "            elif this <= 40:\n",
        "                LOS[c] = \"40%\"\n",
        "            elif this <= 50:\n",
        "                LOS[c] = \"50%\"\n",
        "            elif this <= 60:\n",
        "                LOS[c] = \"60%\"\n",
        "            elif this <= 70:\n",
        "                LOS[c] = \"70%\"\n",
        "            elif this <= 80:\n",
        "                LOS[c] = \"80%\"\n",
        "            elif this <= 90:\n",
        "                LOS[c] = \"90%\"\n",
        "            else:\n",
        "                LOS[c] = \"100%\"\n",
        "        except ValueError:\n",
        "            LOS[c] = '' # Handle cases where conversion to float fails\n",
        "    else:\n",
        "        LOS[c] = ''  # Handle NaN values\n",
        "\n",
        "df2['occupancy Bins'] = pd.Series(LOS)"
      ],
      "metadata": {
        "id": "_UwBpwV0ugjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2"
      ],
      "metadata": {
        "id": "S1rIhuxn35oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3=df2.drop(['detector_group_x','detector_group_y'],axis='columns') #drops the detector group"
      ],
      "metadata": {
        "id": "IX4H3pEEwF-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.to_csv('SixForks Mainline.csv', sep=',') #Write as csv"
      ],
      "metadata": {
        "id": "2cYas7ffHZho"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}